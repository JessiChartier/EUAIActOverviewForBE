# Part 1: Architectural Requirements for Transparency
Table of Contents: 
- Regulatory Context
- Core Architectural Components w/ Reference Diagram
  - Explanability Service
  - Audit Trail Architecture
- Data Flow Architecture
-----
## 1.1 Regulatory Context

The EU AI Act introduces specific transparency requirements based on an AI system's risk classification. As a backend engineer, you need to understand these requirements to implement compliant architectures.

### Transparency Requirements by Risk Level

| Risk Classification | Key Transparency Requirements |
|---------------------|-------------------------------|
| High-Risk Systems | • Complete audit trails<br>• Real-time explainability<br>• Human oversight mechanisms<br>• Comprehensive documentation |
| Limited-Risk Systems | • Basic explainability<br>• Notification of AI interaction<br>• Simplified documentation |
| Minimal-Risk Systems | • Basic documentation<br>• Minimal transparency requirements |

### Backend Engineering Implications

The Act requires you to expose model behavior that may currently be encapsulated. This affects:
- How you structure your model serving infrastructure
- What data you need to store and for how long
- Which APIs you need to expose
- How you handle explainability computation (real-time vs. batch)

## 1.2 Core Architectural Components

A compliant AI system architecture requires specific components to enable transparency. Let's examine each required component and its implementation.

### Reference Architecture Diagram

```
┌───────────────────────────────────────────────────────────────┐
│                  EU AI Act Compliant Architecture              │
├───────────┬─────────────────┬───────────────┬─────────────────┤
│           │                 │               │                 │
│  Client   │   API Gateway   │  Model        │  Explanation    │
│ Requests  │   + Auth        │  Serving      │  Service        │
│           │                 │               │                 │
└─────┬─────┴────────┬────────┴───────┬───────┴────────┬────────┘
      │              │                │                │
      │              │                │                │
┌─────▼──────┐ ┌─────▼──────┐  ┌──────▼─────┐  ┌───────▼───────┐
│            │ │            │  │            │  │               │
│ Request    │ │ Prediction │  │ Metadata   │  │ Explanation   │
│ Logger     │ │ Logger     │  │ Store      │  │ Store         │
│            │ │            │  │            │  │               │
└────────────┘ └────────────┘  └────────────┘  └───────────────┘
      │              │                │                │
      │              │                │                │
      └──────────────┼────────────────┼────────────────┘
                     │                │
               ┌─────▼────────────────▼─────┐
               │                            │
               │  Compliance Audit Service  │
               │                            │
               └────────────────────────────┘
```

### Key Component: Explainability Services

The architecture must include dedicated services for generating and storing explanations.

```python
# ExplanationService interface definition
class ExplanationService:
    """Service for generating and retrieving AI model explanations"""
    
    def generate_explanation(self, model_id: str, prediction_id: str, 
                             inputs: Dict, outputs: Dict) -> Dict:
        """Generate explanation for a prediction"""
        # Implementation depends on model type and explanation method
        pass
    
    def get_explanation(self, prediction_id: str, 
                        format: str = "technical") -> Dict:
        """Retrieve stored explanation by prediction ID
        
        Args:
            prediction_id: Unique identifier for the prediction
            format: Either "technical" (for developers) or 
                   "non_technical" (for end users)
        """
        pass
    
    def list_explanation_methods(self, model_id: str) -> List[str]:
        """List available explanation methods for a model"""
        pass
```

#### Implementation Example: SHAP-based Explainer

```python
class ShapExplanationGenerator(ExplanationGenerator):
    """SHAP-based explanation generator for ML models"""
    
    def __init__(self, model_registry):
        self.model_registry = model_registry
        
    def generate_explanation(self, model_id, inputs, outputs):
        # Get model and its background data
        model = self.model_registry.get_model(model_id)
        background_data = self.model_registry.get_background_data(model_id)
        
        # Create explainer
        explainer = shap.Explainer(model, background_data)
        
        # Calculate SHAP values
        shap_values = explainer(inputs)
        
        # Format explanation according to EU AI Act requirements
        explanation = {
            "prediction_id": str(uuid.uuid4()),
            "model_id": model_id,
            "model_version": model.version,
            "timestamp": datetime.utcnow().isoformat(),
            "explanation_method": "SHAP",
            "feature_importance": self._format_feature_importance(
                model.feature_names, shap_values
            ),
            "technical_explanation": self._create_technical_explanation(shap_values),
            "non_technical_explanation": self._create_human_readable_explanation(
                model.feature_names, shap_values, outputs
            ),
            "visualization_data": self._prepare_visualization_data(shap_values)
        }
        
        return explanation
```

### Key Component: Audit Trail Architecture

The EU AI Act requires comprehensive audit trails, especially for high-risk systems.

```python
# AuditLogger implementation
class AIActAuditLogger:
    """Audit logger compliant with EU AI Act requirements"""
    
    def __init__(self, storage_client, risk_classifier):
        self.storage_client = storage_client
        self.risk_classifier = risk_classifier
        
    def log_prediction_event(self, request_id: str, model_id: str, 
                             inputs: Dict, outputs: Dict,
                             user_id: Optional[str] = None) -> str:
        """Log a prediction event with all required EU AI Act metadata"""
        risk_level = self.risk_classifier.get_risk_level(model_id)
        retention_period = self._get_retention_period(risk_level)
        
        # Create audit record
        audit_record = {
            "event_type": "prediction",
            "request_id": request_id,
            "model_id": model_id,
            "timestamp": datetime.utcnow().isoformat(),
            "risk_level": risk_level,
            "retention_until": (datetime.utcnow() + 
                               timedelta(days=retention_period)).isoformat(),
            "user_id": user_id,
            "input_hash": self._hash_input(inputs),
            "output_hash": self._hash_output(outputs),
        }
        
        # For high-risk systems, we need to store full inputs and outputs
        if risk_level == "HIGH":
            audit_record["inputs"] = self._sanitize_inputs(inputs)
            audit_record["outputs"] = outputs
        
        # Store the audit record
        record_id = self.storage_client.store_audit_record(
            audit_record, retention_period
        )
        
        return record_id
```

## 1.3 Data Flow Architecture

Implementing transparency requires specific data flows through your system.

### Required Data Paths

1. **Input Capture Path**: All API requests → Request Logger → Audit Store
2. **Prediction Path**: Model Input → Model → Output → Prediction Logger → Audit Store  
3. **Explanation Path**: Prediction Data → Explainer → Explanation Store → Audit Store
4. **Oversight Path**: Flagged Predictions → Human Review Interface

### Storage Architecture Considerations

```python
# Example database schema for transparency storage (pseudocode)
CREATE TABLE predictions (
    prediction_id UUID PRIMARY KEY,
    model_id VARCHAR(100) NOT NULL,
    model_version VARCHAR(50) NOT NULL,
    risk_level VARCHAR(20) NOT NULL,
    request_timestamp TIMESTAMP NOT NULL,
    input_hash VARCHAR(64) NOT NULL,
    output_hash VARCHAR(64) NOT NULL,
    user_id VARCHAR(100),
    retention_until TIMESTAMP NOT NULL,
    explanation_id UUID,
    human_review_required BOOLEAN DEFAULT FALSE,
    FOREIGN KEY (explanation_id) REFERENCES explanations(explanation_id)
);

CREATE TABLE explanations (
    explanation_id UUID PRIMARY KEY,
    prediction_id UUID NOT NULL,
    explanation_method VARCHAR(50) NOT NULL,
    technical_explanation JSONB NOT NULL,
    non_technical_explanation TEXT NOT NULL,
    feature_importance JSONB,
    visualization_data JSONB,
    generated_timestamp TIMESTAMP NOT NULL,
    FOREIGN KEY (prediction_id) REFERENCES predictions(prediction_id)
);

# For high-risk systems, we store full inputs and outputs
CREATE TABLE high_risk_details (
    prediction_id UUID PRIMARY KEY,
    inputs JSONB NOT NULL,
    outputs JSONB NOT NULL,
    FOREIGN KEY (prediction_id) REFERENCES predictions(prediction_id)
);
```

### Performance Impact Mitigation

Transparency requirements introduce overhead. Consider these strategies:

1. **Asynchronous Explanation Generation**:
```python
# Example of async explanation generation
async def handle_prediction(inputs):
    # Make prediction synchronously
    prediction = model.predict(inputs)
    prediction_id = str(uuid.uuid4())
    
    # Log prediction
    audit_logger.log_prediction(prediction_id, model_id, inputs, prediction)
    
    # Generate explanation asynchronously
    asyncio.create_task(
        generate_and_store_explanation(prediction_id, inputs, prediction)
    )
    
    return {"prediction": prediction, "prediction_id": prediction_id}

async def generate_and_store_explanation(prediction_id, inputs, outputs):
    explanation = explanation_service.generate_explanation(
        model_id, inputs, outputs
    )
    explanation_store.store_explanation(prediction_id, explanation)
```

2. **Tiered Storage Strategy**:
   - Hot storage: Recent predictions/explanations (fast access)
   - Warm storage: Older data still within retention period
   - Cold storage: Archived data for compliance

3. **Selective Explanation**:
   - Generate detailed explanations only for high-risk predictions
   - Use simpler explanation methods for low-risk predictions
   - Generate explanations on-demand for non-critical systems

### API Design for Transparency

```python
# Example API endpoints for transparency
@app.route("/api/v1/predictions/<prediction_id>", methods=["GET"])
def get_prediction(prediction_id):
    """Get prediction details"""
    prediction = prediction_store.get_prediction(prediction_id)
    return jsonify(prediction)

@app.route("/api/v1/explanations/<prediction_id>", methods=["GET"])
def get_explanation(prediction_id):
    """Get explanation for prediction"""
    format = request.args.get("format", "technical")
    explanation = explanation_service.get_explanation(prediction_id, format)
    return jsonify(explanation)

@app.route("/api/v1/models/<model_id>/card", methods=["GET"])
def get_model_card(model_id):
    """Get model card documentation"""
    model_card = metadata_service.get_model_card(model_id)
    return jsonify(model_card)
```
