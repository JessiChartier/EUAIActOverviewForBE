# Part 2: Implementing Transparency Mechanisms

## Table of Contents
- [2.1 Model Explainability Services](#21-model-explainability-services)
- [2.2 Human Oversight Architecture](#22-human-oversight-architecture)
- [2.3 Audit Trail Architecture](#23-audit-trail-architecture)

## 2.1 Model Explainability Services

Explainability is a core requirement of the EU AI Act. As a backend engineer, you need to implement services that generate explanations for model predictions and expose them through well-defined interfaces.

### Choosing Explainability Methods by Model Type

| Model Type | Recommended Explainability Methods | EU AI Act Requirements |
|------------|-----------------------------------|------------------------|
| Linear Models | • Coefficients<br>• Feature importance | Basic explanation sufficient |
| Tree-based Models | • SHAP values<br>• Feature importance<br>• Decision paths | Detailed path visualization for high-risk |
| Neural Networks | • LIME<br>• Integrated Gradients<br>• Attention visualization | Multiple explanation methods required for high-risk |
| NLP Models | • Attention weights<br>• Token attribution<br>• Counterfactual examples | Both token-level and semantic explanations |
| Computer Vision | • Saliency maps<br>• Grad-CAM<br>• Region attribution | Visual and text explanations |

### Explainability Service Architecture

```python
# Explainability Service Factory
class ExplainabilityServiceFactory:
    """Factory for creating appropriate explainers based on model type"""
    
    @staticmethod
    def create_explainer(model_type: str, model_id: str, model_registry) -> BaseExplainer:
        """Create appropriate explainer based on model type"""
        if model_type == "linear":
            return LinearModelExplainer(model_id, model_registry)
        elif model_type == "tree":
            return TreeModelExplainer(model_id, model_registry)
        elif model_type == "neural_network":
            return DeepLearningExplainer(model_id, model_registry)
        elif model_type == "nlp":
            return NLPModelExplainer(model_id, model_registry)
        elif model_type == "vision":
            return VisionModelExplainer(model_id, model_registry)
        else:
            # Fallback to LIME for unknown model types
            return LimeExplainer(model_id, model_registry)
```

### Example: Implementing a LIME Explainer

```python
class LimeExplainer(BaseExplainer):
    """LIME-based explainer implementation"""
    
    def __init__(self, model_id, model_registry):
        self.model_id = model_id
        self.model = model_registry.get_model(model_id)
        self.feature_names = model_registry.get_feature_names(model_id)
        
    def explain(self, inputs, outputs, num_features=10):
        """Generate LIME explanation"""
        # Create explainer
        explainer = lime.lime_tabular.LimeTabularExplainer(
            self.model.training_data,
            feature_names=self.feature_names,
            class_names=self.model.class_names,
            mode='classification'
        )
        
        # Generate explanation
        exp = explainer.explain_instance(
            inputs, 
            self.model.predict_proba,
            num_features=num_features
        )
        
        # Format explanation according to EU AI Act requirements
        features = exp.as_list()
        feature_importance = {feature: importance for feature, importance in features}
        
        explanation = {
            "method": "LIME",
            "feature_importance": feature_importance,
            "technical_explanation": {
                "intercept": exp.intercept,
                "score": exp.score,
                "local_exp": exp.local_exp,
                "num_features": num_features
            },
            "non_technical_explanation": self._generate_text_explanation(
                features, outputs
            ),
            "visualization_data": self._prepare_visualization_data(exp)
        }
        
        return explanation
    
    def _generate_text_explanation(self, features, outputs):
        """Generate human-readable explanation from LIME features"""
        predicted_class = self.model.class_names[np.argmax(outputs)]
        
        # Get top positive and negative features
        pos_features = [f for f in features if f[1] > 0]
        neg_features = [f for f in features if f[1] < 0]
        
        pos_text = ", ".join([f"{f[0]}" for f in pos_features[:3]])
        neg_text = ", ".join([f"{f[0]}" for f in neg_features[:3]])
        
        explanation = (
            f"The model predicted {predicted_class}. "
            f"The main factors supporting this prediction were: {pos_text}. "
            f"The main factors against this prediction were: {neg_text}."
        )
        
        return explanation
```

### Multi-format Explanation Requirements

The EU AI Act requires explanations in multiple formats:

1. **Technical explanations** (for developers/auditors)
2. **Non-technical explanations** (for end-users)

```python
# Example controller method handling both formats
def get_model_explanation(prediction_id, format_type='technical'):
    """Get explanation in specified format"""
    explanation = explanation_store.get_explanation(prediction_id)
    
    if format_type == 'technical':
        return {
            'method': explanation['method'],
            'feature_importance': explanation['feature_importance'],
            'technical_details': explanation['technical_explanation'],
            'visualization_data': explanation['visualization_data']
        }
    elif format_type == 'non_technical':
        return {
            'explanation': explanation['non_technical_explanation'],
            'key_factors': self._get_top_factors(explanation['feature_importance']),
            'confidence': explanation.get('confidence', 'Not available'),
            'limitations': self._get_explanation_limitations(explanation['method'])
        }
    else:
        raise ValueError(f"Unsupported explanation format: {format_type}")
```

## 2.2 Human Oversight Architecture

For high-risk AI systems, the EU AI Act requires mechanisms for human oversight. As a backend engineer, you need to implement the technical infrastructure that enables this oversight.

### Human Oversight Requirements Matrix

| Risk Level | Oversight Requirement | Technical Implementation |
|------------|----------------------|--------------------------|
| High-Risk | Real-time intervention | Synchronous approval workflow |
| High-Risk | Post-decision review | Flagging and review queue |
| Limited-Risk | Sampling-based review | Automated sampling system |
| Minimal-Risk | Documentation only | Self-service transparency |

### Human-in-the-Loop Architecture

```python
class HumanOversightService:
    """Service for implementing human oversight requirements"""
    
    def __init__(self, risk_classifier, notification_service, storage_client):
        self.risk_classifier = risk_classifier
        self.notification_service = notification_service
        self.storage_client = storage_client
        
    async def process_prediction(self, model_id, inputs, outputs, context=None):
        """Process prediction with appropriate oversight"""
        risk_level = self.risk_classifier.get_risk_level(model_id)
        prediction_id = str(uuid.uuid4())
        
        # Store prediction
        self.storage_client.store_prediction(
            prediction_id, model_id, inputs, outputs, risk_level
        )
        
        # Apply oversight based on risk level
        if risk_level == "HIGH":
            # Determine if human review is needed
            if self._requires_human_review(model_id, inputs, outputs):
                # Place in review queue and notify reviewers
                await self._queue_for_review(prediction_id, model_id, 
                                            inputs, outputs, context)
                return {
                    "status": "pending_review",
                    "prediction_id": prediction_id,
                    "estimated_review_time": self._estimate_review_time()
                }
            
        # For non-high-risk or those not requiring immediate review
        return {
            "status": "completed",
            "prediction_id": prediction_id,
            "outputs": outputs
        }
    
    async def _queue_for_review(self, prediction_id, model_id, inputs, outputs, context):
        """Queue prediction for human review"""
        review_item = {
            "prediction_id": prediction_id,
            "model_id": model_id,
            "inputs": inputs,
            "outputs": outputs,
            "context": context,
            "queued_at": datetime.utcnow().isoformat(),
            "priority": self._calculate_review_priority(model_id, outputs),
            "status": "pending_review"
        }
        
        # Store in review queue
        self.storage_client.add_to_review_queue(review_item)
        
        # Notify appropriate reviewers
        await self.notification_service.notify_reviewers(
            model_id, prediction_id, self._calculate_review_priority(model_id, outputs)
        )
```

### Implementing Review Interfaces

The backend must expose APIs for human reviewers to interact with AI decisions:

```python
# Example Review API endpoints
@app.route("/api/v1/reviews", methods=["GET"])
@require_reviewer_role
def list_pending_reviews():
    """List pending reviews for the authenticated reviewer"""
    reviewer_id = get_authenticated_user_id()
    reviews = review_service.get_pending_reviews(reviewer_id)
    return jsonify(reviews)

@app.route("/api/v1/reviews/<review_id>/approve", methods=["POST"])
@require_reviewer_role
def approve_prediction(review_id):
    """Approve an AI prediction after review"""
    reviewer_id = get_authenticated_user_id()
    review_notes = request.json.get("notes", "")
    
    result = review_service.approve_prediction(
        review_id, reviewer_id, review_notes
    )
    return jsonify(result)

@app.route("/api/v1/reviews/<review_id>/reject", methods=["POST"])
@require_reviewer_role
def reject_prediction(review_id):
    """Reject an AI prediction after review"""
    reviewer_id = get_authenticated_user_id()
    review_notes = request.json.get("notes", "")
    alternative_decision = request.json.get("alternative_decision")
    
    result = review_service.reject_prediction(
        review_id, reviewer_id, review_notes, alternative_decision
    )
    return jsonify(result)
```

### Oversight Metrics Collection

The EU AI Act requires tracking how human oversight is functioning:

```python
class OversightMetricsCollector:
    """Collects metrics on human oversight effectiveness"""
    
    def __init__(self, storage_client):
        self.storage_client = storage_client
        
    def record_review_decision(self, review_id, prediction_id, 
                              reviewer_id, decision, review_time_seconds):
        """Record a human review decision"""
        metrics = {
            "review_id": review_id,
            "prediction_id": prediction_id,
            "reviewer_id": reviewer_id,
            "decision": decision,
            "review_time_seconds": review_time_seconds,
            "timestamp": datetime.utcnow().isoformat()
        }
        
        self.storage_client.store_review_metrics(metrics)
        
    def get_oversight_effectiveness_metrics(self, model_id, 
                                           start_date=None, end_date=None):
        """Get metrics on oversight effectiveness"""
        reviews = self.storage_client.get_reviews(
            model_id, start_date, end_date
        )
        
        # Calculate key metrics
        total_reviews = len(reviews)
        override_rate = sum(1 for r in reviews if r["decision"] == "reject") / total_reviews if total_reviews > 0 else 0
        avg_review_time = sum(r["review_time_seconds"] for r in reviews) / total_reviews if total_reviews > 0 else 0
        
        return {
            "model_id": model_id,
            "period_start": start_date,
            "period_end": end_date,
            "total_reviews": total_reviews,
            "override_rate": override_rate,
            "avg_review_time_seconds": avg_review_time,
            "review_distribution": self._calculate_review_distribution(reviews),
            "reviewer_consistency": self._calculate_reviewer_consistency(reviews)
        }
```

## 2.3 Audit Trail Architecture

The EU AI Act mandates comprehensive audit trails, especially for high-risk systems. Let's implement a compliant audit system.

### Audit Events Schema

```python
# Audit events that must be captured
AUDIT_EVENT_TYPES = {
    "MODEL_DEPLOYMENT": {
        "required_fields": ["model_id", "version", "deployment_environment", 
                           "deployer_id", "deployment_timestamp",
                           "model_card_version", "risk_classification"],
        "retention_period_days": 1825  # 5 years
    },
    "PREDICTION": {
        "required_fields": ["prediction_id", "model_id", "model_version",
                           "timestamp", "input_hash", "output_hash", 
                           "explanation_id", "risk_level"],
        "retention_period_days": 730  # 2 years for standard, longer for high-risk
    },
    "EXPLANATION_GENERATION": {
        "required_fields": ["explanation_id", "prediction_id", "model_id",
                           "explanation_method", "timestamp"],
        "retention_period_days": 730  # 2 years
    },
    "HUMAN_REVIEW": {
        "required_fields": ["review_id", "prediction_id", "reviewer_id",
                           "decision", "timestamp", "review_notes"],
        "retention_period_days": 1825  # 5 years for high-risk reviews
    },
    "DATA_ACCESS": {
        "required_fields": ["access_id", "user_id", "data_type", 
                           "data_id", "access_timestamp", "access_purpose"],
        "retention_period_days": 365  # 1 year
    }
}
```

### Implementing the Audit Service

```python
class AuditService:
    """Comprehensive audit service for EU AI Act compliance"""
    
    def __init__(self, storage_client, encryption_service):
        self.storage_client = storage_client
        self.encryption_service = encryption_service
        
    def log_audit_event(self, event_type, event_data):
        """Log an audit event with appropriate metadata"""
        if event_type not in AUDIT_EVENT_TYPES:
            raise ValueError(f"Unknown audit event type: {event_type}")
            
        # Validate required fields
        required_fields = AUDIT_EVENT_TYPES[event_type]["required_fields"]
        for field in required_fields:
            if field not in event_data:
                raise ValueError(f"Missing required field '{field}' for event type '{event_type}'")
        
        # Add audit metadata
        audit_record = {
            "audit_id": str(uuid.uuid4()),
            "event_type": event_type,
            "timestamp": datetime.utcnow().isoformat(),
            "event_data": event_data,
            "retention_until": (
                datetime.utcnow() + 
                timedelta(days=AUDIT_EVENT_TYPES[event_type]["retention_period_days"])
            ).isoformat()
        }
        
        # For high-risk systems, encrypt sensitive data
        if event_data.get("risk_level") == "HIGH":
            if "input_hash" in event_data and "input_data" in event_data:
                audit_record["event_data"]["input_data"] = self.encryption_service.encrypt(
                    json.dumps(event_data["input_data"])
                )
            if "output_hash" in event_data and "output_data" in event_data:
                audit_record["event_data"]["output_data"] = self.encryption_service.encrypt(
                    json.dumps(event_data["output_data"])
                )
                
        # Store the audit record
        self.storage_client.store_audit_record(audit_record)
        
        return audit_record["audit_id"]
        
    def query_audit_trail(self, filters=None, start_date=None, end_date=None,
                         page=1, page_size=100):
        """Query audit trail with filters and pagination"""
        return self.storage_client.query_audit_records(
            filters, start_date, end_date, page, page_size
        )
        
    def get_audit_timeline(self, entity_id, entity_type="prediction_id"):
        """Get complete timeline for a specific entity"""
        return self.storage_client.get_entity_timeline(entity_id, entity_type)
```

### Implementing Secure Audit Storage

```python
class AuditStorageService:
    """Secure storage service for audit records"""
    
    def __init__(self, db_client, blob_storage_client):
        self.db_client = db_client
        self.blob_storage_client = blob_storage_client
        
    def store_audit_record(self, audit_record):
        """Store audit record with appropriate retention"""
        # For large records, split storage
        if self._is_large_record(audit_record):
            # Store metadata in database
            metadata = {k: v for k, v in audit_record.items() if k != "event_data"}
            metadata["storage_location"] = "blob_storage"
            metadata["blob_id"] = str(uuid.uuid4())
            
            # Store full record in blob storage
            self.blob_storage_client.store_blob(
                metadata["blob_id"],
                json.dumps(audit_record),
                expiration=audit_record["retention_until"]
            )
            
            # Store metadata in DB
            self.db_client.insert_audit_metadata(metadata)
            return metadata["audit_id"]
        else:
            # Store complete record in database
            self.db_client.insert_audit_record(audit_record)
            return audit_record["audit_id"]
            
    def _is_large_record(self, audit_record):
        """Determine if record should be split-stored"""
        serialized = json.dumps(audit_record)
        return len(serialized) > 100 * 1024  # 100KB threshold
```

### Audit Trail Access Control

```python
class AuditAccessController:
    """Controls access to audit records based on roles and permissions"""
    
    def __init__(self, auth_service, audit_storage):
        self.auth_service = auth_service
        self.audit_storage = audit_storage
        
    def get_audit_records(self, user_id, filters=None):
        """Get audit records based on user's permissions"""
        user_roles = self.auth_service.get_user_roles(user_id)
        
        # Define access levels
        if "compliance_officer" in user_roles:
            # Full access to all audit records
            return self.audit_storage.query_audit_records(filters)
        elif "model_owner" in user_roles:
            # Access to records for models they own
            owned_models = self.auth_service.get_user_owned_models(user_id)
            model_filters = {"model_id": {"$in": owned_models}}
            combined_filters = self._combine_filters(filters, model_filters)
            return self.audit_storage.query_audit_records(combined_filters)
        elif "data_scientist" in user_roles:
            # Limited access to aggregated audit data
            return self.audit_storage.get_aggregated_audit_data(filters)
        else:
            # No access
            return {"error": "Insufficient permissions to access audit records"}
```
